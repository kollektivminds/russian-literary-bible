{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2a369691-b00d-49da-9f85-3fcb684738eb",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Synodal Bible & Mother"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6613b100-20dd-489e-857f-bcf145d1f8d0",
   "metadata": {},
   "source": [
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96f94134-774b-4ba2-b67d-5ff8672f5738",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Code summary of below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c94aee-67e0-4fae-a70a-4890d5916c66",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81150c35-805c-479c-b0cc-754bd314ef7c",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba5391cc-af4e-4028-9c1b-cb2585b2e396",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b1ba77-b72c-4599-9e23-c62589a37bee",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from backend import *\n",
    "import multiprocessing as mp\n",
    "from multiprocessing import Pool, Process\n",
    "import pickle\n",
    "num_cores = int(mp.cpu_count()-2)\n",
    "print(f\"Number of cores : {num_cores}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f14af7f-82be-48ec-bf2c-098c3fddcaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a700f02-da3e-4628-85fc-60868cad4423",
   "metadata": {},
   "source": [
    "### Definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c199dd-c3f6-4a0a-8db5-3cafb94f80fe",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "texts = '../texts/fiction/utf8/'\n",
    "\n",
    "libCols = ['author','pub_year','title','text']\n",
    "tokenOHCO = ['w_id','part_num','para_num', 'sent_num', 'token_num']\n",
    "tokenCols = ['p_id', 'start', 'stop', 'text', 'token_id', 'head_id', 'rel', 'pos', 'lemma', 'anim', 'aspect', \\\n",
    "             'case', 'degree', 'gender', 'mood', 'number', 'person', 'tense', 'verb_form', 'voice']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2777424b-faa0-4d69-95be-18dc507c6bb3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motherTokenDf = pd.read_pickle('./proc/MotherTokendf.pkl')\n",
    "bibleTokenDf = pd.read_pickle('./proc/BibleTokenDf.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b62cd7d-a549-4467-85d7-f36b3bfe0acc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# add stopword boolean, True if all attribute columns are null\n",
    "motherTokenDf['stopword'] = ~motherTokenDf.loc[:, tokenCols[9:]].any(axis=1)\n",
    "TokenDfIdx = pd.Index(range(1, (motherTokenDf.shape[0]+1)), name='id')\n",
    "motherTokenDf.index = TokenDfIdx# = TokenDf.set_index(['p_id', 'token_id'])\n",
    "\n",
    "bibleTokenDf['stopword'] = ~bibleTokenDf.loc[:, tokenCols[9:]].any(axis=1)\n",
    "TokenDfIdx = pd.Index(range(1, (bibleTokenDf.shape[0]+1)), name='id')\n",
    "bibleTokenDf.index = TokenDfIdx# = TokenDf.set_index(['p_id', 'token_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f29e8068-3d72-4c44-ab7f-5db318ec8ab2",
   "metadata": {},
   "source": [
    "## Bible and Mother Token Dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690fa276-b51c-4b57-81f2-349667b13aea",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bibleTokenDf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0b6c2b-3970-42bc-a9c5-411525698da3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motherTokenDf#.set_index(['p_id','token_id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "639dcd23-e439-4fe5-9240-ad4f2f868c5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "lemmaDf = bibleTokenDf[['lemma']].loc[bibleTokenDf.stopword == False].join(motherTokenDf[['lemma']].loc[motherTokenDf.stopword == False], on='id', lsuffix='_bible', rsuffix='_mother')\n",
    "lemmaDf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e011f0-94b5-4930-b913-059c89dfb83b",
   "metadata": {},
   "source": [
    "## Shared Lemmas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e08bca-f309-438e-95a8-bda27c5cdf83",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "bible_lemmas = bibleTokenDf.loc[bibleTokenDf.stopword == False].lemma\n",
    "mother_lemmas = motherTokenDf.loc[motherTokenDf.stopword == False].lemma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f020ddf-4b97-4e55-92d3-8b614da87203",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "common_lemmas = []\n",
    "common_lemmas_count = 0\n",
    "for lemma in mother_lemmas.unique():\n",
    "    #print(lemma)\n",
    "    if lemma in bible_lemmas.unique():\n",
    "        common_lemmas.append(lemma)\n",
    "        common_lemmas_count += 1\n",
    "        \n",
    "print(f\"Shared unique words between Mother and the Bible: \"+str(common_lemmas_count)+\", that's ~\"+str(round(((common_lemmas_count/len(lemmaDf.lemma_mother.unique()))*100),2))+\"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ece887f-ac69-445d-9743-3e3c5b76034b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def ListSplit(lst, numGroups, sort=True): \n",
    "    \"\"\"Takes a list and a number and splits the \n",
    "    list into evenly divided n groups (as much as possible)\"\"\"\n",
    "    # choose to sort list by ascending\n",
    "    if sort: \n",
    "        lst.sort()\n",
    "    # get length of list given to sort\n",
    "    listLen = len(lst)\n",
    "    # groupLen is the maximum number of items per group to allow for the most groups <= numGroups in a list (numGroups-1 OR numGroups)\n",
    "    groupLen = (listLen//numGroups) + (listLen % numGroups > 0)\n",
    "    # yield generator object of nested listed with length of numGroups \n",
    "    for i in range(0,len(lst), groupLen): \n",
    "        yield lst[i:i+groupLen]\n",
    "\n",
    "def token_window(lemma_array, window_size):\n",
    "    token_window_array = np.array([])\n",
    "    token_window_array = np.array([np.array(lemma_array[i:i+window_size]) for i in range(len(lemma_array)-(window_size-1))])\n",
    "    return token_window_array\n",
    "\n",
    "def token_window_mp(lemma_array, window_size):\n",
    "    #token_window_array = np.array([])\n",
    "    token_window_array = enumerate([list(lemma_array[i:i+window_size]) for i in range(len(lemma_array)-(window_size-1))])\n",
    "    return token_window_array\n",
    "\n",
    "def get_sim_count(child_window_array, parent_window_array, order_matters=False):\n",
    "    work_sim_count = 0\n",
    "    work_sim = []\n",
    "    for child_window in child_window_array:\n",
    "        child_sim = {}\n",
    "        #print(f\"child window: {child_window}\")\n",
    "        child_window_sim_total_count = 0\n",
    "        for parent_window in parent_window_array:\n",
    "            #print(f\"parent window: {parent_window}\")\n",
    "            child_window_sim_sub_count = 0\n",
    "            for i in range(len(child_window)):\n",
    "                if order_matters:\n",
    "                    if child_window[1][i] == parent_window[1][i]:\n",
    "                        child_window_sim_total_count+=1\n",
    "                        child_window_sim_sub_count+=1\n",
    "                else:\n",
    "                    if child_window[1][i] in parent_window[1]:\n",
    "                        #print(f\"{child_window[i]} is in parent!\")\n",
    "                        child_window_sim_total_count+=1\n",
    "                        child_window_sim_sub_count+=1\n",
    "                    #else:\n",
    "                        #print(f\"{child_window[i]} is not in parent!\")\n",
    "            child_sim.update({parent_window[0]: child_window_sim_sub_count})\n",
    "        #print(f\"{child_window}: {child_window_sim_total_count}\")\n",
    "        work_sim.append({child_window_sim_total_count: child_sim})\n",
    "    return work_sim\n",
    "\n",
    "def dna_test_windows(child_lemmas, parent_lemmas, window_size=10):\n",
    "    sim_counter = 0\n",
    "    #sim_list = np.array([])\n",
    "    parent_window_array = token_window(parent_lemmas, window_size)\n",
    "    child_window_array = token_window(child_lemmas, window_size)\n",
    "    sim_list = get_sim_count(child_window_array, parent_window_array)\n",
    "    #print(sim_list)\n",
    "    return sim_list\n",
    "\n",
    "def dna_test_windows_mp(child_lemmas, parent_lemmas, window_size=10):\n",
    "    #child_lemmas, parent_lemmas = cnp_lemmas[0], cnp_lemmas[1]\n",
    "    #print(f\"Child lemmas: {child_lemmas}\")\n",
    "    #print(f\"Parent lemmas: {parent_lemmas[:10]}\")\n",
    "    sim_counter = 0\n",
    "    sim_list = np.array([])\n",
    "    parent_window_array = token_window_mp(parent_lemmas, window_size)\n",
    "    child_window_array = token_window_mp(child_lemmas, window_size)\n",
    "    sim_list = get_sim_count(child_window_array, parent_window_array)\n",
    "    return sim_list\n",
    "\n",
    "def window_count_sim_mp(child_window_array, parent_window_array, order_matters=False):\n",
    "    sim_count_tuples = {}\n",
    "    for (c_sub_array_id, c_sub_array_list) in child_window_array:\n",
    "        c_sub_sim_count = 0\n",
    "        for (p_sub_array_id, p_sub_array_list) in parent_window_array:\n",
    "            c_p_sim_count = 0\n",
    "            for i in range(len(c_sub_array_list)):\n",
    "                if order_matters:\n",
    "                    if c_sub_array_list[i] == p_sub_array_list[i]:\n",
    "                        c_sub_sim_count += 1\n",
    "                        c_p_sim_count += 1\n",
    "                else:\n",
    "                    if c_sub_array_list[i] in p_sub_array_list:\n",
    "                        c_sub_sim_count += 1\n",
    "                        c_p_sim_count += 1\n",
    "            sim_count_tuples[(c_sub_array_id, p_sub_array_id)] = c_p_sim_count\n",
    "    return sim_count_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83ddcb1e-467a-4755-8a82-9160f2b7b02d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def list_split(lst, numGroups, sort=True): \n",
    "    \"\"\"Takes a list and a number and splits the \n",
    "    list into evenly divided n groups (as much as possible)\"\"\"\n",
    "    # choose to sort list by ascending\n",
    "    if sort: \n",
    "        lst.sort()\n",
    "    # get length of list given to sort\n",
    "    listLen = len(lst)\n",
    "    # groupLen is the maximum number of items per group to allow for the most groups <= numGroups in a list (numGroups-1 OR numGroups)\n",
    "    groupLen = (listLen//numGroups) + (listLen % numGroups > 0)\n",
    "    # yield generator object of nested listed with length of numGroups \n",
    "    for i in range(0,len(lst), groupLen): \n",
    "        yield lst[i:i+groupLen]\n",
    "\n",
    "def token_window_mp(lemma_array, window_size):\n",
    "    #token_window_array = np.array([])\n",
    "    token_window_array = enumerate([list(lemma_array[i:i+window_size]) for i in range(len(lemma_array)-(window_size-1))])\n",
    "    return token_window_array\n",
    "\n",
    "def window_count_sim_mp(child_window_array, parent_window_array, order_matters=False):\n",
    "    sim_count_tuples = {}\n",
    "    for (c_sub_array_id, c_sub_array_list) in child_window_array:\n",
    "        c_sub_sim_count = 0\n",
    "        for (p_sub_array_id, p_sub_array_list) in parent_window_array:\n",
    "            c_p_sim_count = 0\n",
    "            for i in range(len(c_sub_array_list)):\n",
    "                if order_matters:\n",
    "                    if c_sub_array_list[i] == p_sub_array_list[i]:\n",
    "                        c_sub_sim_count += 1\n",
    "                        c_p_sim_count += 1\n",
    "                else:\n",
    "                    if c_sub_array_list[i] in p_sub_array_list:\n",
    "                        c_sub_sim_count += 1\n",
    "                        c_p_sim_count += 1\n",
    "            sim_count_tuples[(c_sub_array_id, p_sub_array_id)] = c_p_sim_count\n",
    "    return sim_count_tuples\n",
    "\n",
    "def dna_test_windows_mp(child_lemmas, parent_lemmas, window_size=20):\n",
    "    sim_list = {}\n",
    "    parent_window_array = token_window_mp(parent_lemmas, window_size)\n",
    "    child_window_array = token_window_mp(child_lemmas, window_size)\n",
    "    sim_list = window_count_sim_mp(child_window_array, parent_window_array)\n",
    "    return sim_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b47d068-79f7-4b50-82fd-d16122162343",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "if __name__ == \"__main__\":\n",
    "    pool = Pool(processes=num_cores)\n",
    "    #workerAssignments = ((x, bible_lemmas) for x in ListSplit(mother_lemmas[:198], num_cores, False))\n",
    "    starmap_list = list(zip(list_split(child_window_array, num_cores), list_split(parent_window_array, num_cores)))\n",
    "    print(f\"Splitting into {num_cores} groups\")\n",
    "    start_time = time.time()\n",
    "    sim_tuples = pool.starmap(window_count_sim_mp, starmap_list, chunksize=2)\n",
    "    print(f\"Finished in {round(time.time()-start_time, 3)} seconds.\")\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "    #print(sim_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0752ba0-63e0-42ae-bdfc-04760c0cf161",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "motherTokenDf.loc[motherTokenDf.lemma == \"эммаус\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97e31093-299e-47d7-a60f-34e86a02e027",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(sim_tuples[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50dfc082-d838-4bb4-845d-7be5a2e8d2e9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d488635e-7a54-4403-beb1-a495064f464b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_sim_tuples = {}\n",
    "for t in sim_tuples:\n",
    "    total_sim_tuples.update(t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22983cbd-9cdb-4618-9778-2ab569ab2fbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(total_sim_tuples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f3ec4e-0db4-40a3-9d78-7e8aaf324546",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "with open('./proc/sim_tuples.pkl', 'wb') as file:\n",
    "    pickle.dump(total_sim_tuples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bec0ede-c9c8-46a6-b7b0-1c684f7dacc2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "child_window_array = list(token_window_mp(mother_lemmas, 20))\n",
    "parent_window_array = list(token_window_mp(bible_lemmas, 20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45a96cd-fc04-4d7d-85a9-6b90f6e466e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "[x for x in parent_window_array]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c6af48-baf7-4379-891f-90412ef18e27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "child_window_array = list(token_window_mp(mother_lemmas, 20))\n",
    "parent_window_array = list(token_window_mp(bible_lemmas, 20))\n",
    "#[x for x in parent_window_array]\n",
    "sim_count_tuples = window_count_sim(child_window_array, parent_window_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdf01e82-ac89-41e9-8ffc-3a594886c7b2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sim_count_tuples.items()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73461604-db09-4a68-b7af-66cecd9cace4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame([x for tup in sim_count_tuples for x in tup], columns=['window_vocab', 'sim_count']).sort_values(by='sim_count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62036625-db0c-4e15-bdc9-8d07e3d1211a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "with open('./proc/sim_tuples.pkl', 'wb') as file:\n",
    "    pickle.dump(sim_tuples, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fece4ec9-7350-4d04-8dfb-be145383dd28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pd.DataFrame(sim_tuples, columns=['window_vocab', 'sim_count']).locsort_values(by='sim_count', axis=0, ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da474da-4ff9-4dfb-ac01-2626061862d4",
   "metadata": {},
   "source": [
    "How do Gor'kii's *Mother* and the Synodal Bible overlap? \n",
    "windows of 3-5 words with 1-2+ of overlap"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "diss",
   "language": "python",
   "name": "diss"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "ea13d3b8728aada41f8a13ca417c5cc9432fd2a5df9ca31286f49d0e6820a779"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
